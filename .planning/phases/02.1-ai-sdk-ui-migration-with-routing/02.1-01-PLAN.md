---
phase: 02.1-ai-sdk-ui-migration-with-routing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/lib/ai/webllm-transport.ts
  - src/hooks/useAIChat.ts
autonomous: true
requirements:
  - CHAT-03
must_haves:
  truths:
    - "AI SDK UI @ai-sdk/react package is installed"
    - "WebLLM custom transport bridges WebLLM streaming with AI SDK UI"
    - "useAIChat hook wraps useChat with WebLLM transport"
    - "True streaming works without batch-then-stream pattern"
  artifacts:
    - path: "src/lib/ai/webllm-transport.ts"
      provides: "Custom ChatTransport implementation for WebLLM"
      exports: ["WebLLMTransport"]
    - path: "src/hooks/useAIChat.ts"
      provides: "React hook for AI SDK UI with WebLLM transport"
      exports: ["useAIChat"]
  key_links:
    - from: "useAIChat hook"
      to: "WebLLMTransport"
      via: "transport option in useChat"
      pattern: "transport: new WebLLMTransport()"
    - from: "WebLLMTransport"
      to: "inferenceManager"
      via: "generate() method"
      pattern: "inferenceManager.generate(messages)"
---

<objective>
Install AI SDK UI and create the custom WebLLM transport that bridges WebLLM's streaming inference with AI SDK UI's state management.

Purpose: This is the foundation for all AI SDK UI integration. Without the transport, AI SDK UI cannot communicate with WebLLM.
Output: Working transport layer and hook that can stream tokens from WebLLM through AI SDK UI.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-ai-sdk-ui-migration-with-routing/02.1-RESEARCH.md

@/Users/poak/Documents/personal-project/Lokul/src/lib/ai/inference.ts
@/Users/poak/Documents/personal-project/Lokul/src/lib/ai/models.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK UI dependencies</name>
  <files>package.json</files>
  <action>
    Install @ai-sdk/react and ai packages:
    ```bash
    npm install @ai-sdk/react ai
    ```

    These packages provide the useChat hook and types needed for AI SDK UI integration.
    Verify installation by checking node_modules/@ai-sdk/react exists.
  </action>
  <verify>npm list @ai-sdk/react ai</verify>
  <done>Dependencies installed and listed in package.json</done>
</task>

<task type="auto">
  <name>Task 2: Create WebLLM custom transport</name>
  <files>src/lib/ai/webllm-transport.ts</files>
  <action>
    Create a custom ChatTransport implementation that bridges WebLLM with AI SDK UI.

    The transport must:
    1. Implement the ChatTransport interface from @ai-sdk/react
    2. Accept modelId in constructor options
    3. In sendMessage(): Convert UIMessage[] to WebLLM format, call inferenceManager.generate(), yield tokens
    4. Handle errors by yielding { type: 'error', error: message }
    5. In abort(): Call inferenceManager.abort() and abortController.abort()
    6. Use AbortController for cancellation support

    Key implementation details from research:
    - WebLLM's generate() returns AsyncGenerator<string> for tokens
    - AI SDK UI expects AsyncGenerator<{ type: string; content?: string; error?: string }>
    - Map 'user' | 'assistant' | 'system' roles directly (they match WebLLM)
    - Yield { type: 'text', content: token } for each token
    - Yield { type: 'finish' } when stream completes

    Reference inference.ts for the inferenceManager API:
    - inferenceManager.generate(messages) yields tokens
    - inferenceManager.abort() stops generation
    - inferenceManager.isLoaded() checks if model is ready

    DO NOT modify inference.ts - work with its existing API.
  </action>
  <verify>TypeScript compiles without errors: npm run type-check</verify>
  <done>WebLLMTransport class exists with sendMessage and abort methods</done>
</task>

<task type="auto">
  <name>Task 3: Create useAIChat hook</name>
  <files>src/hooks/useAIChat.ts</files>
  <action>
    Create a wrapper hook that configures useChat from @ai-sdk/react with the WebLLM transport.

    The hook should:
    1. Import { useChat } from '@ai-sdk/react'
    2. Import { WebLLMTransport } from '@/lib/ai/webllm-transport'
    3. Accept options: conversationId, modelId, initialMessages
    4. Create WebLLMTransport instance with modelId
    5. Call useChat({ id: conversationId, transport, initialMessages })
    6. Return the full useChat result (messages, sendMessage, status, error, etc.)

    This hook is the bridge between the existing model store and AI SDK UI.
    The conversationId becomes the chat ID for AI SDK UI state management.

    Type the options interface explicitly:
    ```typescript
    interface UseAIChatOptions {
      conversationId: string;
      modelId: string;
      initialMessages?: UIMessage[];
    }
    ```
  </action>
  <verify>TypeScript compiles without errors: npm run type-check</verify>
  <done>useAIChat hook exports a function that returns useChat result with WebLLM transport</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run `npm run type-check` - should pass with no errors
2. Verify webllm-transport.ts exports WebLLMTransport class
3. Verify useAIChat.ts exports useAIChat function
4. Check that imports resolve correctly (@ai-sdk/react types available)
</verification>

<success_criteria>
- @ai-sdk/react and ai packages installed
- WebLLMTransport implements ChatTransport interface correctly
- useAIChat hook wraps useChat with WebLLM transport
- TypeScript compilation passes
- No runtime errors on import
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-ai-sdk-ui-migration-with-routing/02.1-01-SUMMARY.md`
</output>
