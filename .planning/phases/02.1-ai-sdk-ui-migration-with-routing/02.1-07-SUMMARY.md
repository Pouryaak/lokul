---
phase: 02.1-ai-sdk-ui-migration-with-routing
plan: 07
type: execute
subsystem: chat-interface
completed: 2026-02-18
duration: 3 min
tasks: 3
files_created: 0
files_modified: 4
key-decisions:
  - "Moved performance components into ChatLayout for better component composition"
  - "Added token tracking functions to metrics module for TPS calculation"
  - "Model selector in sidebar creates new conversation on change (user decision)"
tech-stack:
  added: []
  patterns:
    - "Performance components integrated into layout component"
    - "Token tracking during streaming for real-time TPS"
    - "Model selector with automatic new conversation creation"
key-files:
  created: []
  modified:
    - src/components/chat-layout/ChatLayout.tsx
    - src/App.tsx
    - src/lib/performance/metrics.ts
    - src/lib/ai/webllm-transport.ts
    - src/components/chat-layout/AppSidebar.tsx
---

# Phase 2.1 Plan 07: Performance Integration Summary

## Overview

Integrated performance monitoring and status indicators into the new chat interface. All existing features now work with the AI SDK UI migration.

## What Was Built

### 1. Performance Components in Chat Layout

**StatusIndicator** - Fixed bottom-left position, always visible
- Shows offline/online capability status
- Indicates when app is ready for offline use
- Displays current network state

**PerformancePanel** - Toggleable from top-right button
- Shows GPU status and device name
- Displays memory usage with visual bar
- System health indicator (Green/Yellow/Red)
- Updates every 5 seconds

### 2. Token Tracking for TPS Calculation

Added new functions to `metrics.ts`:
- `startTokenTracking()` - Begins tracking session
- `recordToken()` - Records each token during streaming
- `stopTokenTracking()` - Returns final metrics (tokens, duration, TPS)
- `getCurrentTPS()` - Real-time tokens per second calculation

Updated `WebLLMTransport` to:
- Start tracking when generation begins
- Record each token as it streams
- Stop tracking and record final metrics when complete

### 3. Model Selector in Sidebar

Added model dropdown to AppSidebar:
- Shows all available models (Quick, Smart, Genius)
- Displays model name and description
- Changing model creates new conversation automatically
- Uses Cpu icon for visual identification

## Files Modified

| File | Changes |
|------|---------|
| `src/components/chat-layout/ChatLayout.tsx` | Added StatusIndicator, PerformancePanel toggle, and state management |
| `src/App.tsx` | Removed duplicate performance components from ChatLayoutWrapper |
| `src/lib/performance/metrics.ts` | Added token tracking functions and InferenceMetrics interface |
| `src/lib/ai/webllm-transport.ts` | Integrated token tracking into streaming loop |
| `src/components/chat-layout/AppSidebar.tsx` | Added model selector dropdown with change handler |

## Verification

- [x] Status indicator visible in bottom-left corner
- [x] Performance toggle button appears top-right
- [x] Performance panel shows/hides on button click
- [x] Token tracking starts when generation begins
- [x] Each token recorded during streaming
- [x] Final metrics recorded when generation completes
- [x] Model selector shows all available models
- [x] Model change creates new conversation
- [x] TypeScript compiles without errors

## Deviations from Plan

None - plan executed exactly as written.

## Architecture Notes

### Performance Component Placement

The performance components were moved from `App.tsx`'s `ChatLayoutWrapper` into `ChatLayout.tsx` itself. This provides better component composition and ensures the performance UI is part of the layout rather than wrapping it.

### Token Tracking Design

Token tracking uses module-level state in `metrics.ts` rather than React state. This is intentional because:
1. Tokens stream at high frequency (avoid React render overhead)
2. Metrics are read by PerformancePanel every 5 seconds (not real-time)
3. Simple counter pattern is sufficient for TPS calculation

### Model Selector Behavior

Per user decision: "Changing model creates a new chat with the chosen model." The implementation:
1. Loads the new model via `loadModel()`
2. Creates new conversation via `createNewConversation()`
3. Triggers `onNewChat()` callback to navigate to `/chat`

## Commits

1. `bc85ccc` - feat(02.1-07): integrate performance components into chat layout
2. `c8206b6` - feat(02.1-07): wire performance metrics to AI SDK UI streaming
3. `de321ba` - feat(02.1-07): add model selector to chat sidebar

## Self-Check: PASSED

- [x] All modified files exist
- [x] All commits recorded
- [x] TypeScript compiles without errors
- [x] No breaking changes to existing functionality
