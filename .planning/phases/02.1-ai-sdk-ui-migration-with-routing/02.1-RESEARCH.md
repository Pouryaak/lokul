# Phase 02.1: AI SDK UI Migration with Routing - Research

**Researched:** 2026-02-18
**Domain:** React chat architecture with AI SDK UI, ai-elements, React Router, WebLLM integration
**Confidence:** HIGH

## Summary

This research covers the migration from Lokul's custom chat implementation to industry-standard patterns using AI SDK UI and ai-elements components, with proper URL routing for conversations.

**Key Findings:**

1. **AI SDK UI (@ai-sdk/react)** provides the `useChat` hook which is the industry standard for React chat applications. It handles streaming, state management, and message persistence patterns out of the box.

2. **ai-elements** is a shadcn/ui-based component library specifically designed for AI chat interfaces. It provides `Conversation`, `Message`, `PromptInput`, and `Loader` components that integrate seamlessly with AI SDK UI.

3. **Custom Transport Pattern**: For client-side only inference with WebLLM, AI SDK UI supports custom transports via the `transport` option in `useChat`. This allows WebLLM to be used as the inference backend while leveraging AI SDK UI's state management.

4. **React Router v7** (already used in the project) supports nested routes and dynamic parameters (`/chat/:id`) through standard patterns with `useParams()` and `Outlet`.

5. **WebLLM Streaming**: WebLLM supports true streaming via `engine.chat.completions.create({ stream: true })` which returns an AsyncGenerator, compatible with AI SDK UI's streaming expectations when bridged correctly.

**Primary Recommendation:**
Use `@ai-sdk/react` with a custom transport that wraps WebLLM's streaming API, combined with ai-elements components (`Conversation`, `Message`, `PromptInput`) for the UI layer, and React Router for `/chat` and `/chat/:id` routing.

---

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

**Routing structure**
- Root URL (/) always shows the 11-section landing page (no redirect)
- /chat without ID creates a new empty conversation automatically
- Individual conversations at /chat/[id] pattern
- Non-existent conversation ID → redirect to /chat with warning toast
- Browser history pushes new entry when auto-creating at /chat (user can navigate back)

**AI SDK UI integration**
- Use best industry approach (research ai-elements vs other options)
- Preference: clean, maintainable, industry best practice
- Do not reinvent the wheel if SDKs can handle it
- True streaming from WebLLM (not batch-then-stream)
- Error surfaced as object in conversation state (standard AI SDK pattern)

**Model switching behavior**
- For v1: Changing model creates a new chat with the chosen model
- History doesn't transfer when switching models (clean slate)

**Component architecture**
- Selective adoption strategy: Use components from different libraries as needed
- ai-elements components for core chat functionality where appropriate
- Message threading: Flat chronological (not grouped or thread-based)
- Sidebar: Use @blocks/sidebar-02 from blocks.so (collapsible)

**Design language**
- Keep Lokul design language (orange/cream theme)
- Prioritize maintainability over strict design adherence
- Research ai-elements documentation for customization approach
- Apply Lokul colors/spacing to ai-elements components

**Chat UX behavior**
- Auto-scroll: Use ai-elements Conversation component's built-in handling
- Loading state: Shimmering "Thinking..." text effect
- Input behavior: Allow typing during generation
- Send button becomes cancel/stop button during generation
- Input component: Use @blocks/ai-02 from blocks.so as reference
- Empty state: Welcome screen with suggested prompts

### Claude's Discretion
- Exact AI SDK UI provider implementation (research best approach)
- ai-elements customization specifics
- Specific shimmer animation implementation
- Welcome screen content and layout
- Error display formatting

### Deferred Ideas (OUT OF SCOPE)

**Model switching with context transfer**
- For v1, changing models creates a fresh conversation
- Future version: Compact chat history and pass it to the new model so it can catch up
- This is NOT in scope for Phase 2.1 — consider for Phase 4 (Memory System)
</user_constraints>

---

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| CHAT-01 | User can type messages in an auto-resizing input field | ai-elements PromptInput component with PromptInputTextarea provides auto-resizing |
| CHAT-02 | User can send messages with Enter key or Send button | PromptInputSubmit handles both Enter and button submission |
| CHAT-03 | AI responses stream token-by-token without UI lag | WebLLM `stream: true` returns AsyncGenerator; AI SDK UI `useChat` with custom transport handles streaming |
| CHAT-04 | Messages render with Markdown formatting | ai-elements MessageResponse with Streamdown library handles markdown |
| CHAT-05 | Code blocks display with syntax highlighting | ai-elements code-block component available; existing Shiki can be integrated |
| CHAT-06 | User can copy any message to clipboard with one click | MessageAction component with copy functionality |
| CHAT-07 | User can regenerate the last AI response | `useChat` hook provides `regenerate` function |
| CHAT-08 | User can clear the current conversation | Route navigation to `/chat` creates new empty conversation |
</phase_requirements>

---

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| @ai-sdk/react | ^1.x | React hooks for chat state and streaming | Industry standard from Vercel; handles complex streaming state |
| ai-elements | latest | Pre-built shadcn/ui components for AI chat | Official Vercel library; designed specifically for AI SDK UI |
| react-router | ^7.x | URL routing with /chat/:id pattern | Already in project; supports nested routes and dynamic params |
| @mlc-ai/web-llm | ^0.2.80 | Local inference engine | Already in project; supports streaming via AsyncGenerator |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| streamdown | latest | Markdown rendering in messages | ai-elements uses this internally for MessageResponse |
| lucide-react | ^0.475.0 | Icons for UI | Already in project; ai-elements uses lucide icons |
| shadcn/ui | latest | Base component library | ai-elements builds on shadcn/ui components |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| ai-elements | Custom components | Custom gives full control but loses built-in AI SDK UI integration |
| @ai-sdk/react | Zustand custom store | Custom store requires reimplementing streaming state management |
| React Router | TanStack Router | TanStack Router has better type safety but React Router is already integrated |

**Installation:**
```bash
# AI SDK UI
npm install @ai-sdk/react ai

# ai-elements components (via CLI)
npx ai-elements@latest add conversation
npx ai-elements@latest add message
npx ai-elements@latest add prompt-input
npx ai-elements@latest add loader
npx ai-elements@latest add code-block

# Or via shadcn
npx shadcn@latest add https://ai-sdk.dev/elements/api/registry/all.json
```

---

## Architecture Patterns

### Recommended Project Structure
```
src/
├── components/
│   ├── chat/
│   │   ├── ChatPage.tsx           # Main chat page with routing
│   │   ├── ChatLayout.tsx         # Layout with sidebar
│   │   └── WelcomeScreen.tsx      # Empty state with suggestions
│   ├── ai-elements/               # ai-elements components (auto-installed)
│   │   ├── conversation.tsx
│   │   ├── message.tsx
│   │   ├── prompt-input.tsx
│   │   └── loader.tsx
│   └── sidebar/
│       └── AppSidebar.tsx         # @blocks/sidebar-02 based
├── hooks/
│   ├── useChat.ts                 # Re-export/wrapper for @ai-sdk/react
│   └── useWebLLMTransport.ts      # Custom transport for WebLLM
├── lib/
│   ├── ai/
│   │   ├── webllm-transport.ts    # Custom AI SDK transport
│   │   └── models.ts              # Model configurations
│   └── storage/
│       └── conversations.ts       # IndexedDB operations
├── routes/
│   ├── Root.tsx                   # Root layout with landing
│   ├── ChatRoute.tsx              # /chat route
│   └── ChatDetailRoute.tsx        # /chat/:id route
└── App.tsx                        # Router configuration
```

### Pattern 1: Custom WebLLM Transport for AI SDK UI
**What:** Implement a custom `ChatTransport` that bridges WebLLM's streaming API with AI SDK UI's expectations.

**When to use:** When using client-side only inference (no server) with AI SDK UI.

**Example:**
```typescript
// lib/ai/webllm-transport.ts
import { ChatTransport, UIMessage } from '@ai-sdk/react';
import { inferenceManager } from './inference';

export class WebLLMTransport implements ChatTransport {
  async *sendMessage(options: {
    messages: UIMessage[];
    body?: Record<string, unknown>;
  }): AsyncGenerator<{
    type: string;
    content?: string;
    error?: string;
  }> {
    const { messages, body } = options;
    const modelId = body?.model as string;

    // Ensure model is loaded
    if (!inferenceManager.isLoaded() || inferenceManager.getCurrentModel() !== modelId) {
      await inferenceManager.initialize(modelId);
    }

    try {
      // Convert UIMessages to WebLLM format
      const inferenceMessages = messages.map(m => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content
      }));

      // Stream from WebLLM
      const stream = inferenceManager.generate(inferenceMessages);

      for await (const token of stream) {
        yield {
          type: 'text',
          content: token
        };
      }
    } catch (error) {
      yield {
        type: 'error',
        error: error instanceof Error ? error.message : 'Unknown error'
      };
    }
  }

  abort(): void {
    inferenceManager.abort();
  }
}
```

### Pattern 2: Route-Based Conversation Loading
**What:** Use React Router's `useParams` and `useEffect` to load conversations when the URL changes.

**When to use:** For `/chat/:id` pattern where conversations are loaded from IndexedDB.

**Example:**
```typescript
// routes/ChatDetailRoute.tsx
import { useParams, useNavigate } from 'react-router';
import { useEffect } from 'react';
import { useChat } from '@ai-sdk/react';
import { getConversation } from '@/lib/storage/conversations';
import { WebLLMTransport } from '@/lib/ai/webllm-transport';

export function ChatDetailRoute() {
  const { id } = useParams<{ id: string }>();
  const navigate = useNavigate();
  const { setMessages, status } = useChat({
    transport: new WebLLMTransport()
  });

  useEffect(() => {
    async function loadConversation() {
      if (!id) return;

      const conversation = await getConversation(id);
      if (!conversation) {
        // Redirect to /chat with toast warning
        navigate('/chat', {
          state: { warning: 'Conversation not found' }
        });
        return;
      }

      // Convert to UIMessage format
      const uiMessages = conversation.messages.map(m => ({
        id: m.id,
        role: m.role,
        content: m.content,
        createdAt: new Date(m.timestamp)
      }));

      setMessages(uiMessages);
    }

    loadConversation();
  }, [id, navigate, setMessages]);

  return <ChatInterface />;
}
```

### Pattern 3: Auto-Creating New Conversations
**What:** When navigating to `/chat` without an ID, automatically create a new conversation and redirect to `/chat/:id`.

**When to use:** Per user decision - /chat creates new conversation automatically.

**Example:**
```typescript
// routes/ChatRoute.tsx
import { useEffect } from 'react';
import { useNavigate } from 'react-router';
import { createConversation } from '@/lib/storage/conversations';
import { useModelStore } from '@/store/modelStore';

export function ChatRoute() {
  const navigate = useNavigate();
  const currentModel = useModelStore(s => s.currentModel);

  useEffect(() => {
    async function createNewChat() {
      const modelId = currentModel?.id || 'default-model';
      const conversation = createConversation(modelId);
      await saveConversation(conversation);

      // Push to history so back button works
      navigate(`/chat/${conversation.id}`, { replace: false });
    }

    createNewChat();
  }, [navigate, currentModel]);

  return <LoadingState />;
}
```

### Pattern 4: ai-elements Component Composition
**What:** Compose ai-elements components with Lokul's design system.

**When to use:** For all chat UI components.

**Example:**
```typescript
// components/chat/ChatInterface.tsx
import {
  Conversation,
  ConversationContent,
  ConversationEmptyState,
  ConversationScrollButton,
} from '@/components/ai-elements/conversation';
import {
  Message,
  MessageContent,
  MessageResponse,
  MessageActions,
  MessageAction,
} from '@/components/ai-elements/message';
import {
  PromptInput,
  PromptInputTextarea,
  PromptInputSubmit,
} from '@/components/ai-elements/prompt-input';
import { Loader } from '@/components/ai-elements/loader';
import { useChat } from '@ai-sdk/react';

export function ChatInterface() {
  const { messages, sendMessage, status } = useChat();

  return (
    <div className="flex h-full flex-col bg-[#FFF8F0]">
      <Conversation className="flex-1">
        <ConversationContent>
          {messages.length === 0 ? (
            <ConversationEmptyState
              icon={<MessageSquare className="size-12 text-[#FF6B35]" />}
              title="Start a conversation"
              description="Type a message below to begin chatting with Lokul"
            />
          ) : (
            messages.map((message) => (
              <Message key={message.id} from={message.role}>
                <MessageContent>
                  {message.parts.map((part, i) =>
                    part.type === 'text' ? (
                      <MessageResponse key={i}>{part.text}</MessageResponse>
                    ) : null
                  )}
                </MessageContent>
                <MessageActions>
                  <MessageAction
                    tooltip="Copy message"
                    onClick={() => copyToClipboard(message.content)}
                  >
                    <CopyIcon />
                  </MessageAction>
                </MessageActions>
              </Message>
            ))
          )}
          {status === 'streaming' && <Loader />}
        </ConversationContent>
        <ConversationScrollButton />
      </Conversation>

      <PromptInput onSubmit={(msg) => sendMessage({ text: msg.text })}>
        <PromptInputTextarea placeholder="Message Lokul..." />
        <PromptInputSubmit />
      </PromptInput>
    </div>
  );
}
```

### Anti-Patterns to Avoid
- **Don't** use `useChat` without a transport - it defaults to `/api/chat` endpoint which doesn't exist in client-only apps
- **Don't** manually manage streaming state - let AI SDK UI handle it via the transport
- **Don't** store messages in both Zustand and AI SDK UI - use AI SDK UI as source of truth
- **Don't** use `window.history` directly - use React Router's `useNavigate` for proper state management

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Streaming state management | Custom Zustand store for streaming | AI SDK UI `useChat` hook | Handles edge cases: abort, error recovery, reconnection, backpressure |
| Message scrolling | Custom scroll-to-bottom logic | ai-elements `Conversation` component | Built-in auto-scroll with user scroll detection pause |
| Input auto-resize | Manual textarea height calculation | ai-elements `PromptInputTextarea` | Handles edge cases: max-height, paste, window resize |
| Copy to clipboard | `navigator.clipboard` directly | ai-elements `MessageAction` | Includes tooltip feedback, error handling |
| Chat message parts | Custom message format | AI SDK UI `UIMessage` with `parts` | Standard format supports text, reasoning, sources, tool calls |
| Route-based state | Manual URL parsing | React Router `useParams` | Type-safe, handles encoding, works with browser APIs |

**Key insight:** The AI SDK UI ecosystem exists specifically to solve the complex problems of chat UI state management. Custom implementations inevitably miss edge cases around streaming, error handling, and state synchronization.

---

## Common Pitfalls

### Pitfall 1: Transport Not Initialized Before First Message
**What goes wrong:** First message fails because WebLLM model isn't loaded yet.

**Why it happens:** WebLLM requires explicit model initialization before generation, unlike server-based APIs.

**How to avoid:**
- Initialize model in route loader or parent component
- Show loading state until model is ready
- Handle initialization in transport's `sendMessage` with proper error messages

**Warning signs:** "Engine not initialized" errors, first message always fails.

### Pitfall 2: Message ID Mismatch Between Storage and UI
**What goes wrong:** Regenerate or edit operations target wrong message.

**Why it happens:** AI SDK UI generates its own message IDs which may differ from IndexedDB IDs.

**How to avoid:**
- Use AI SDK UI's message ID as the source of truth
- Map to storage format on save, not on load
- Include `conversationId` in message metadata

**Warning signs:** Regenerate updates wrong message, duplicate messages appear.

### Pitfall 3: Streaming Not Stopping on Route Change
**What goes wrong:** AI continues generating after user navigates away.

**Why it happens:** Transport abort isn't called when component unmounts.

**How to avoid:**
```typescript
useEffect(() => {
  return () => {
    transport.abort();
  };
}, [transport]);
```

**Warning signs:** Background token generation continues, memory usage grows.

### Pitfall 4: Race Condition on Rapid Send/Stop/Send
**What goes wrong:** Messages appear out of order or streaming shows wrong content.

**Why it happens:** Multiple concurrent streams without proper abort handling.

**How to avoid:**
- Always await abort before starting new generation
- Use AI SDK UI's built-in status checking (`status === 'ready'`)
- Disable input while status is not 'ready'

**Warning signs:** Interleaved message content, duplicate assistant responses.

### Pitfall 5: URL State and Store State Out of Sync
**What goes wrong:** Refreshing `/chat/:id` shows wrong conversation or empty state.

**Why it happens:** Store is cleared on refresh but URL still has ID.

**How to avoid:**
- Always load from IndexedDB based on URL param, not store
- Treat URL as source of truth for conversation ID
- Use React Router's loader pattern for initial data fetch

**Warning signs:** Empty chat after refresh, wrong conversation loaded.

---

## Code Examples

### WebLLM Transport Implementation
```typescript
// lib/ai/webllm-transport.ts
import type { ChatTransport, UIMessage } from '@ai-sdk/react';
import { inferenceManager } from './inference';

export interface WebLLMTransportOptions {
  modelId: string;
  onModelLoad?: () => void;
}

export class WebLLMTransport implements ChatTransport {
  private abortController: AbortController | null = null;

  constructor(private options: WebLLMTransportOptions) {}

  async *sendMessage(options: {
    messages: UIMessage[];
    body?: Record<string, unknown>;
  }): AsyncGenerator<{
    type: 'text' | 'error' | 'finish';
    content?: string;
    error?: string;
  }> {
    this.abortController = new AbortController();
    const { signal } = this.abortController;

    try {
      // Ensure model is loaded
      if (!inferenceManager.isLoaded()) {
        await inferenceManager.initialize(this.options.modelId);
        this.options.onModelLoad?.();
      }

      // Convert to WebLLM format
      const inferenceMessages = options.messages.map(m => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content
      }));

      // Stream tokens
      const stream = inferenceManager.generate(inferenceMessages);

      for await (const token of stream) {
        if (signal.aborted) break;
        yield { type: 'text', content: token };
      }

      yield { type: 'finish' };
    } catch (error) {
      yield {
        type: 'error',
        error: error instanceof Error ? error.message : 'Generation failed'
      };
    }
  }

  abort(): void {
    this.abortController?.abort();
    inferenceManager.abort();
  }
}
```

### Router Configuration
```typescript
// App.tsx
import { BrowserRouter, Routes, Route, Navigate } from 'react-router';
import { RootLayout } from './routes/RootLayout';
import { LandingPage } from './routes/LandingPage';
import { ChatLayout } from './routes/ChatLayout';
import { ChatRoute } from './routes/ChatRoute';
import { ChatDetailRoute } from './routes/ChatDetailRoute';

export function App() {
  return (
    <BrowserRouter>
      <Routes>
        <Route path="/" element={<RootLayout />}>
          <Route index element={<LandingPage />} />
        </Route>

        <Route path="chat" element={<ChatLayout />}>
          <Route index element={<ChatRoute />} />
          <Route path=":id" element={<ChatDetailRoute />} />
        </Route>
      </Routes>
    </BrowserRouter>
  );
}
```

### Chat Page with ai-elements
```typescript
// routes/ChatDetailRoute.tsx
import { useParams } from 'react-router';
import { useChat } from '@ai-sdk/react';
import { WebLLMTransport } from '@/lib/ai/webllm-transport';
import {
  Conversation,
  ConversationContent,
  ConversationScrollButton,
} from '@/components/ai-elements/conversation';
import {
  Message,
  MessageContent,
  MessageResponse,
} from '@/components/ai-elements/message';
import {
  PromptInput,
  PromptInputTextarea,
  PromptInputSubmit,
} from '@/components/ai-elements/prompt-input';
import { Loader } from '@/components/ai-elements/loader';

export function ChatDetailRoute() {
  const { id } = useParams<{ id: string }>();
  const { messages, sendMessage, status } = useChat({
    transport: new WebLLMTransport({ modelId: id }),
    id // Conversation ID for persistence
  });

  return (
    <div className="flex h-full flex-col">
      <Conversation className="flex-1">
        <ConversationContent>
          {messages.map((message) => (
            <Message key={message.id} from={message.role}>
              <MessageContent>
                {message.parts
                  .filter((p): p is { type: 'text'; text: string } => p.type === 'text')
                  .map((part, i) => (
                    <MessageResponse key={i}>{part.text}</MessageResponse>
                  ))}
              </MessageContent>
            </Message>
          ))}
          {status === 'streaming' && <Loader />}
        </ConversationContent>
        <ConversationScrollButton />
      </Conversation>

      <PromptInput
        onSubmit={(msg) => sendMessage({ text: msg.text })}
        className="border-t p-4"
      >
        <PromptInputTextarea placeholder="Message Lokul..." />
        <PromptInputSubmit
          status={status === 'streaming' ? 'streaming' : 'ready'}
        />
      </PromptInput>
    </div>
  );
}
```

### Customizing ai-elements with Lokul Theme
```typescript
// components/ai-elements/message.tsx (override)
import { Message as BaseMessage } from '@/components/ai-elements/message';
import { cn } from '@/lib/utils';

export function Message({ className, from, ...props }: MessageProps) {
  return (
    <BaseMessage
      from={from}
      className={cn(
        // Lokul-specific styling
        from === 'user' && 'max-w-[85%]',
        from === 'assistant' && 'max-w-[90%]',
        className
      )}
      {...props}
    />
  );
}

// Override MessageContent for Lokul colors
export function MessageContent({ className, ...props }: MessageContentProps) {
  return (
    <BaseMessageContent
      className={cn(
        'group-[.is-user]:bg-[#FF6B35]',
        'group-[.is-user]:text-white',
        'group-[.is-user]:rounded-2xl',
        'group-[.is-assistant]:bg-white',
        'group-[.is-assistant]:border',
        'group-[.is-assistant]:border-gray-200',
        className
      )}
      {...props}
    />
  );
}
```

---

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Custom Zustand store for chat | AI SDK UI `useChat` | 2024 | Standardized streaming, less boilerplate |
| Manual fetch with useEffect | Transport pattern | 2024 | Cleaner separation of data fetching |
| react-markdown directly | ai-elements MessageResponse | 2025 | Better integration with streaming |
| Custom scroll handling | Conversation component | 2025 | Built-in auto-scroll with pause on user scroll |

**Deprecated/outdated:**
- Direct WebLLM engine calls in components: Use transport pattern instead
- Manual message array management: Use AI SDK UI's message state
- Custom streaming parsers: AI SDK UI handles stream protocol

---

## Open Questions

1. **How to handle model switching mid-conversation?**
   - What we know: User decision is to create new chat on model switch
   - What's unclear: Should we warn user about losing context?
   - Recommendation: Show confirmation modal when switching models with existing messages

2. **What's the best shimmer animation approach?**
   - What we know: ai-elements has a Loader component
   - What's unclear: Can it be customized for "Thinking..." text effect?
   - Recommendation: Extend ai-elements Loader with custom CSS animation

3. **How to integrate Shiki with ai-elements MessageResponse?**
   - What we know: ai-elements uses Streamdown for markdown
   - What's unclear: Whether Streamdown supports custom code block renderers
   - Recommendation: Research Streamdown API or override MessageResponse for code blocks

4. **What's the persistence pattern with AI SDK UI?**
   - What we know: AI SDK UI has experimental persistence API
   - What's unclear: Whether to use it or manually sync to IndexedDB
   - Recommendation: Manual sync to IndexedDB for now - more control over schema

---

## Sources

### Primary (HIGH confidence)
- `/vercel/ai-elements` (Context7) - Component APIs, installation, usage patterns
- `/websites/ai-sdk_dev_elements` (Context7) - Conversation, Message, PromptInput examples
- https://ai-sdk.dev/docs/ai-sdk-ui/chatbot - AI SDK UI documentation
- https://reactrouter.com/start/library/routing - React Router v7 patterns

### Secondary (MEDIUM confidence)
- WebLLM README (GitHub) - Streaming API patterns
- ai-elements CLI documentation - Component installation commands

### Tertiary (LOW confidence)
- blocks.so documentation - Sidebar patterns (could not verify exact API)

---

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Verified with Context7 and official docs
- Architecture: HIGH - Based on official AI SDK UI patterns
- Pitfalls: MEDIUM - Some based on general React patterns, not all verified with AI SDK UI specifically

**Research date:** 2026-02-18
**Valid until:** 2026-03-18 (30 days for stable libraries)

---

## RESEARCH COMPLETE

**Phase:** 02.1 - AI SDK UI Migration with Routing
**Confidence:** HIGH

### Key Findings

1. **AI SDK UI with custom transport is the correct pattern** for client-side only inference. The `WebLLMTransport` bridges WebLLM's AsyncGenerator streaming with AI SDK UI's expectations.

2. **ai-elements components** provide production-ready chat UI that integrates seamlessly with AI SDK UI. They handle complex UX patterns (auto-scroll, input resizing, action buttons) out of the box.

3. **React Router nested routes** cleanly separate the landing page (/) from chat routes (/chat, /chat/:id) with proper parameter handling via `useParams()`.

4. **True streaming is achievable** - WebLLM's `stream: true` returns an AsyncGenerator that can be yielded from the transport to AI SDK UI.

5. **Theme customization** is possible via CSS class overrides on ai-elements components, preserving Lokul's orange/cream design language.

### File Created
`.planning/phases/02.1-ai-sdk-ui-migration-with-routing/02.1-RESEARCH.md`

### Confidence Assessment
| Area | Level | Reason |
|------|-------|--------|
| Standard Stack | HIGH | Verified with Context7 (ai-elements) and official docs (AI SDK UI) |
| Architecture | HIGH | Based on official patterns from Vercel documentation |
| Pitfalls | MEDIUM | Some extrapolated from general React patterns, not all AI SDK UI specific |

### Open Questions
1. Streamdown customization for Shiki integration
2. Exact shimmer animation implementation details
3. Persistence API vs manual IndexedDB sync

### Ready for Planning
Research complete. Planner can now create PLAN.md files with specific implementation tasks.
