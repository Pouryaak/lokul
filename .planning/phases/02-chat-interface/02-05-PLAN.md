---
phase: 02-chat-interface
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/ai/models.ts
  - src/lib/ai/inference.ts
  - src/store/modelStore.ts
autonomous: true
gap_closure: true
requirements:
  - MODEL-01
  - MODEL-07
must_haves:
  truths:
    - "User can load the app and start chatting after model download completes"
    - "Model download succeeds without 'failed to download model' error"
    - "Error messages include useful diagnostic information"
  artifacts:
    - path: "src/lib/ai/models.ts"
      provides: "Model configurations with correct WebLLM IDs"
      contains: "-MLC suffix on all model IDs"
    - path: "src/lib/ai/inference.ts"
      provides: "Inference manager with error logging"
      contains: "console.error for model initialization failures"
    - path: "src/store/modelStore.ts"
      provides: "Model store with preserved error context"
      contains: "Full error message including original error details"
  key_links:
    - from: "src/lib/ai/models.ts"
      to: "WebLLM prebuilt config"
      via: "Model ID matching"
      pattern: "phi-2-q4f16_1-MLC|Llama-3.2-3B-Instruct-q4f16_1-MLC|Mistral-7B-Instruct-v0.3-q4f16_1-MLC"
---

<objective>
Fix the model download failure caused by incorrect model ID format. Update all model IDs to include the required "-MLC" suffix that WebLLM v0.2.80 expects, and add better error logging to diagnose future model loading issues.

Purpose: Unblock Phase 2 UAT testing by fixing the blocker that prevents users from reaching the chat interface.
Output: Updated model configurations with correct IDs and improved error diagnostics.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02-chat-interface/02-UAT.md
@.planning/debug/DEBUG-model-download-failed.md

## Gap Context

The UAT test 1 failed because clicking "Start Chatting" shows "Download failed, failed to download model" after 1 second. Root cause identified: Model ID format mismatch - all three model IDs are missing the required "-MLC" suffix that WebLLM v0.2.80 expects.

**Current (broken) model IDs:**
- `phi-2-q4f16_1` (line 34)
- `llama-3.2-3b-q4f16_1` (line 48)
- `mistral-7b-q4f16_1` (line 62)

**Correct WebLLM model IDs:**
- `phi-2-q4f16_1-MLC`
- `Llama-3.2-3B-Instruct-q4f16_1-MLC`
- `Mistral-7B-Instruct-v0.3-q4f16_1-MLC`

**Additional issues to fix:**
- Missing error logging around CreateWebWorkerMLCEngine call in inference.ts
- Error handling in modelStore.ts loses original error context
</context>

<tasks>

<task type="auto">
  <name>Fix model IDs in models.ts</name>
  <files>src/lib/ai/models.ts</files>
  <action>
    Update all three model IDs to include the required "-MLC" suffix that WebLLM v0.2.80 expects:

    1. Line 34: Change `id: "phi-2-q4f16_1"` to `id: "phi-2-q4f16_1-MLC"`
    2. Line 48: Change `id: "llama-3.2-3b-q4f16_1"` to `id: "Llama-3.2-3B-Instruct-q4f16_1-MLC"`
    3. Line 62: Change `id: "mistral-7b-q4f16_1"` to `id: "Mistral-7B-Instruct-v0.3-q4f16_1-MLC"`

    Note: The modelUrl values are already correct with the -MLC suffix. Only the id fields need updating.
  </action>
  <verify>grep -n "-MLC" src/lib/ai/models.ts | head -10</verify>
  <done>All three model IDs end with "-MLC" suffix and match WebLLM's prebuilt configuration</done>
</task>

<task type="auto">
  <name>Add error logging to inference.ts</name>
  <files>src/lib/ai/inference.ts</files>
  <action>
    Add proper error logging around the CreateWebWorkerMLCEngine call to help diagnose model loading failures in the future.

    Around lines 79-90, wrap the engine creation in a try-catch with detailed logging:

    Before the existing try-catch at line 87, add error logging inside the catch block:
    - Log the full error to console.error with context about which model failed
    - Include the modelId in the error message
    - Log the error stack if available

    The catch block at lines 87-90 should be enhanced to log the error before re-throwing.
  </action>
  <verify>grep -n "console.error" src/lib/ai/inference.ts</verify>
  <done>Error logging exists around CreateWebWorkerMLCEngine with model ID context</done>
</task>

<task type="auto">
  <name>Improve error handling in modelStore.ts</name>
  <files>src/store/modelStore.ts</files>
  <action>
    Improve the error handling in the loadModel action (lines 112-121) to preserve more error context.

    Current code only extracts err.message. Update to:
    - Include the original error message
    - Add context about which model failed to load
    - Include the error name/type if available
    - Format as: "Failed to load model '{modelId}': {error.message}"

    This provides users with actionable error messages and helps with debugging.
  </action>
  <verify>grep -A 5 "catch (err)" src/store/modelStore.ts</verify>
  <done>Error message includes model ID and original error context</done>
</task>

</tasks>

<verification>
- [ ] Model IDs in src/lib/ai/models.ts all end with "-MLC" suffix
- [ ] Error logging exists in src/lib/ai/inference.ts around CreateWebWorkerMLCEngine
- [ ] Error messages in src/store/modelStore.ts include model ID context
- [ ] TypeScript compiles without errors: `npm run type-check`
- [ ] ESLint passes: `npm run lint`
</verification>

<success_criteria>
1. All three model IDs match WebLLM's prebuilt configuration exactly
2. Model download can proceed without "failed to download model" error
3. Error logging provides diagnostic information for future debugging
4. Code passes type-check and lint
</success_criteria>

<output>
After completion, create `.planning/phases/02-chat-interface/02-05-SUMMARY.md`
</output>
