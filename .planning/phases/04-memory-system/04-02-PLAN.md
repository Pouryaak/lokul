---
phase: 04-memory-system
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/lib/memory/context-builder.ts
  - src/lib/memory/compaction.ts
  - src/lib/memory/eviction.ts
  - src/lib/ai/chat-transport.ts
  - src/hooks/useMemoryExtraction.ts
autonomous: true
requirements:
  - MEM-03
  - MEM-04
must_haves:
  truths:
    - "Context window usage is monitored and stays under 80% threshold"
    - "Memory facts are injected into system prompt with proper formatting"
    - "Two-stage compaction triggers at 80% threshold (trim history, then reduce memory)"
    - "Token budgeting reserves 25% of remaining context for memory (min 150, max 500)"
    - "Extraction triggers every 5 messages and at end-of-session"
  artifacts:
    - path: "src/lib/memory/context-builder.ts"
      provides: "Context injection with token budgeting"
      exports: ["buildContextWithMemory", "calculateMemoryBudget", "formatMemoryForPrompt"]
    - path: "src/lib/memory/compaction.ts"
      provides: "Two-stage compaction at 80% threshold"
      exports: ["compactContext", "stageOneTrimHistory", "stageTwoReduceMemory"]
    - path: "src/lib/memory/eviction.ts"
      provides: "Eviction score calculation and fact pruning"
      exports: ["calculateEvictionScore", "evictFactsIfNeeded", "pruneExpiredFacts"]
    - path: "src/hooks/useMemoryExtraction.ts"
      provides: "Periodic extraction trigger logic"
      exports: ["useMemoryExtraction"]
    - path: "src/lib/ai/chat-transport.ts"
      provides: "Integration with chat transport for context injection"
      contains: "memory context in system prompt"
  key_links:
    - from: "src/lib/ai/chat-transport.ts"
      to: "src/lib/memory/context-builder.ts"
      via: "buildContextWithMemory call"
      pattern: "system prompt includes formatted memory"
    - from: "src/hooks/useMemoryExtraction.ts"
      to: "src/lib/memory/extraction.ts"
      via: "extractFacts call"
      pattern: "trigger on message count"
    - from: "src/lib/memory/context-builder.ts"
      to: "tokenlens"
      via: "token counting"
      pattern: "tokensRemaining|percentOfContextUsed"
---

<objective>
Implement intelligent context management: token budgeting for memory injection, two-stage compaction when approaching token limits, and eviction strategies to keep the system running smoothly without crashes.

Purpose: Without proper context management, the AI will hit token limits and crash. This plan ensures the system gracefully handles long conversations by compacting context and managing memory within budget.
Output: Context builder with token budgeting, compaction logic, eviction scoring, and extraction triggers.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-memory-system/04-CONTEXT.md
@.planning/phases/04-memory-system/04-RESEARCH.md

# Dependencies from 04-01
@src/lib/memory/types.ts
@src/lib/memory/extraction.ts
@src/lib/storage/memory.ts

# Integration points
@src/lib/ai/chat-transport.ts
@src/types/result.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Context Builder with Token Budgeting</name>
  <files>
    src/lib/memory/context-builder.ts
  </files>
  <action>
    Create src/lib/memory/context-builder.ts with token-aware context assembly:

    1. Import dependencies:
       - tokenlens for token counting (tokensRemaining, percentOfContextUsed, modelMeta)
       - MemoryFact from @/lib/memory/types
       - Result types from @/types/result

    2. Implement calculateMemoryBudget:
       ```typescript
       export function calculateMemoryBudget(
         modelId: string,
         conversationTokens: number,
         reserveOutput: number = 1000 // Reserve for response
       ): number {
         const meta = modelMeta(modelId);
         const contextWindow = meta.contextWindow;

         // Available after conversation and output reserve
         const available = contextWindow - conversationTokens - reserveOutput;

         // Memory gets 25% of remaining, clamped to min/max
         const idealBudget = Math.floor(available * 0.25);
         return Math.max(150, Math.min(500, idealBudget));
       }
       ```

    3. Implement selectMemoriesForInjection:
       ```typescript
       export function selectMemoriesForInjection(
         memories: MemoryFact[],
         budgetTokens: number,
         currentMessage?: string // For keyword relevance boost
       ): MemoryFact[]
       ```
       Selection logic:
       - Sort by priority: pinned first, then project > preference > identity
       - Within same priority: higher confidence first, then more recent
       - Add memories until budget is reached (with safety margin)
       - Track running token count, stop when within 50 tokens of budget

    4. Implement formatMemoryForPrompt:
       ```typescript
       export function formatMemoryForPrompt(memories: MemoryFact[]): string
       ```
       Format (from CONTEXT.md):
       ```
       ## What you know about this user

       Current work:
       - Building a local-first AI chat app with WebLLM
       - Currently implementing a memory extraction system

       Preferences:
       - Prefers direct answers without preamble
       - Uses TypeScript, minimal abstraction

       About user:
       - Solo developer based in Copenhagen
       ```
       Rules:
       - Group by category (project, preference, identity)
       - Skip empty categories (no header if no facts)
       - Use "- " prefix for each fact
       - No attribution or "why" context

    5. Implement buildContextWithMemory (main entry point):
       ```typescript
       export function buildContextWithMemory(
         messages: Message[],
         memories: MemoryFact[],
         modelId: string,
         options?: {
           currentMessage?: string;
           baseSystemPrompt?: string;
         }
       ): {
         messages: Message[];
         memoryTokens: number;
         totalTokens: number;
         percentUsed: number;
       }
       ```
       Logic:
       - Calculate conversation tokens using tokenlens
       - Calculate memory budget
       - Select memories for injection
       - Format memory section
       - Combine: [system with memory, ...conversation messages]
       - Return token statistics for monitoring

    6. Add token estimation helper:
       ```typescript
       // Estimate tokens for a string (fallback if tokenlens fails)
       export function estimateTokens(text: string): number
       ```
       Use rough heuristic: chars / 4 (standard approximation)
  </action>
  <verify>
    - calculateMemoryBudget returns values between 150-500
    - formatMemoryForPrompt produces correctly formatted sections
    - buildContextWithMemory includes memory in system prompt
    - TypeScript compiles without errors
  </verify>
  <done>
    - calculateMemoryBudget implements 25% rule with min/max clamps
    - selectMemoriesForInjection respects priority and budget
    - formatMemoryForPrompt groups by category with proper headers
    - buildContextWithMemory is the main integration point
  </done>
</task>

<task type="auto">
  <name>Task 2: Two-Stage Compaction Logic</name>
  <files>
    src/lib/memory/compaction.ts
  </files>
  <action>
    Create src/lib/memory/compaction.ts implementing the 80% threshold compaction:

    1. Define constants:
       ```typescript
       const COMPACTION_THRESHOLD = 0.8; // 80% of context window
       const MESSAGES_TO_KEEP_START = 2; // First exchange (system + user)
       const MESSAGES_TO_KEEP_END = 6; // Last 6 messages
       ```

    2. Implement shouldCompact:
       ```typescript
       export function shouldCompact(
         totalTokens: number,
         contextWindow: number
       ): boolean {
         return totalTokens / contextWindow >= COMPACTION_THRESHOLD;
       }
       ```

    3. Implement stageOneTrimHistory (middle-out trimming):
       ```typescript
       export function stageOneTrimHistory(
         messages: Message[]
       ): { messages: Message[]; trimmed: number }
       ```
       Logic:
       - Keep first MESSAGES_TO_KEEP_START messages (system + first user)
       - Keep last MESSAGES_TO_KEEP_END messages
       - Remove everything in between
       - Return trimmed count for logging

       Example:
       ```
       Before: [S, U1, A1, U2, A2, U3, A3, U4, A4, U5, A5, U6, A6]
       After:  [S, U1, A4, U5, A5, U6, A6] (kept S,U1 and last 6)
       ```

    4. Implement stageTwoReduceMemory:
       ```typescript
       export function stageTwoReduceMemory(
         memories: MemoryFact[],
         targetBudget: number
       ): { memories: MemoryFact[]; reduction: number }
       ```
       Logic (from CONTEXT.md):
       - Drop all identity facts (lowest priority)
       - Halve preferences (keep only high-confidence > 0.8)
       - Keep all project facts intact
       - Always keep pinned facts regardless of category
       - Return reduction count for logging

    5. Implement compactContext (orchestrates both stages):
       ```typescript
       export function compactContext(
         messages: Message[],
         memories: MemoryFact[],
         modelId: string,
         options?: {
           baseSystemPrompt?: string;
         }
       ): {
         messages: Message[];
         memories: MemoryFact[];
         stage: "none" | "stage1" | "stage2";
         memoryTokens: number;
         totalTokens: number;
         percentUsed: number;
       }
       ```
       Logic:
       1. Calculate current token usage
       2. If under threshold: return as-is with stage: "none"
       3. Apply Stage 1: Trim history
       4. Recalculate tokens, if still over threshold:
       5. Apply Stage 2: Reduce memory injection
       6. Return final state with stage indicator

    6. Add logging in DEV mode:
       - Log when compaction triggers
       - Log how many messages were trimmed
       - Log memory reduction details

    7. Export a convenience hook for components:
       ```typescript
       export function useCompactionStatus(): {
         isCompacting: boolean;
         lastCompaction: { stage: string; savedTokens: number } | null;
       }
       ```
  </action>
  <verify>
    - stageOneTrimHistory keeps first 2 and last 6 messages
    - stageTwoReduceMemory drops identity and halves preferences
    - compactContext applies stages sequentially only when needed
    - shouldCompact returns true at exactly 80% threshold
  </verify>
  <done>
    - Stage 1 compaction trims conversation history middle-out
    - Stage 2 compaction reduces memory injection
    - Compaction only triggers at 80% threshold
    - Pinned facts are protected from Stage 2
  </done>
</task>

<task type="auto">
  <name>Task 3: Eviction and Extraction Triggers</name>
  <files>
    src/lib/memory/eviction.ts
    src/hooks/useMemoryExtraction.ts
  </files>
  <action>
    1. Create src/lib/memory/eviction.ts:

       Define constants from CONTEXT.md:
       ```typescript
       const HARD_CAP = 150;
       const PRUNE_THRESHOLD = 120;
       const MAX_PINNED = 10;
       const CATEGORY_EXPIRY = {
         project: 60 * 24 * 60 * 60 * 1000, // 60 days
         preference: 180 * 24 * 60 * 60 * 1000, // 180 days
         identity: 365 * 24 * 60 * 60 * 1000, // 365 days
       };
       const CATEGORY_WEIGHTS = {
         project: 0.8,
         preference: 0.3,
         identity: 0.5,
       };
       ```

       Implement calculateEvictionScore:
       ```typescript
       export function calculateEvictionScore(fact: MemoryFact): number {
         const ageInDays = (Date.now() - fact.lastSeen) / (24 * 60 * 60 * 1000);
         const weight = CATEGORY_WEIGHTS[fact.category];
         return (ageInDays * weight) / fact.confidence;
       }
       ```
       Higher score = evict first.

       Implement evictFactsIfNeeded:
       ```typescript
       export async function evictFactsIfNeeded(
         options?: { signal?: AbortSignal }
       ): Promise<Result<{ evicted: number; remaining: number }, AppError>>
       ```
       Logic:
       - Get all facts count
       - If count <= PRUNE_THRESHOLD: return early
       - Calculate eviction scores for all non-pinned facts
       - Sort by score descending
       - Evict (delete) facts until count <= HARD_CAP
       - Return counts for logging

       Implement pruneExpiredFacts:
       ```typescript
       export async function pruneExpiredFacts(
         options?: { signal?: AbortSignal }
       ): Promise<Result<number, AppError>> // Returns count pruned
       ```
       Logic:
       - For each category, check lastSeen against expiry
       - Delete facts older than category expiry
       - Skip pinned facts
       - Return count pruned

    2. Create src/hooks/useMemoryExtraction.ts:

       ```typescript
       export function useMemoryExtraction(
         conversationId: string,
         messages: Message[]
       ): {
         triggerExtraction: () => void;
         isExtracting: boolean;
         lastExtraction: number | null;
       }
       ```

       Implementation:
       - Track message count since last extraction
       - Track last extraction timestamp
       - Trigger extraction when:
         a) Message count reaches 5 since last extraction
         b) End-of-session detected (beforeunload, visibilitychange)
       - Use setTimeout to defer extraction (don't block UI)
       - Call extractFacts from 04-01
       - Call processExtractedFacts to save results

       End-of-session detection:
       ```typescript
       useEffect(() => {
         const handleBeforeUnload = () => {
          // beforeunload cannot reliably run async work â€”
          // visibilitychange to "hidden" is the primary end-of-session trigger.
          // beforeunload only handles the synchronous cleanup path.
          // Use navigator.sendBeacon pattern if server sync is ever added.
          // For now: rely on visibilitychange which fires before beforeunload
          // and has more time to complete async work.
           triggerExtraction();
         };
         const handleVisibilityChange = () => {
           if (document.visibilityState === "hidden") {
             triggerExtraction();
           }
         };
         window.addEventListener("beforeunload", handleBeforeUnload);
         document.addEventListener("visibilitychange", handleVisibilityChange);
         return () => {
           window.removeEventListener("beforeunload", handleBeforeUnload);
           document.removeEventListener("visibilitychange", handleVisibilityChange);
         };
       }, []);
       ```

       Extraction trigger:
       ```typescript
       const triggerExtraction = useCallback(() => {
         if (messages.length < 2) return; // Need some context
         if (isExtractingRef.current) return; // Deduplicate

         isExtractingRef.current = true;
         setIsExtracting(true);

         // Defer to avoid blocking
         setTimeout(async () => {
           try {
             const engine = await getEngine(); // From model store
             const result = await extractFacts(engine, messages);
             if (result.kind === "ok" && result.value.length > 0) {
               await processExtractedFacts(result.value, conversationId);
             }
           } finally {
             isExtractingRef.current = false;
             setIsExtracting(false);
             setLastExtraction(Date.now());
           }
         }, 0);
       }, [messages, conversationId]);
       ```

    3. Add cleanup effect for component unmount:
       - Trigger extraction on unmount (end-of-session for conversation)
  </action>
  <verify>
    - calculateEvictionScore produces higher scores for older, lower-confidence facts
    - evictFactsIfNeeded respects HARD_CAP and skips pinned facts
    - useMemoryExtraction triggers every 5 messages
    - End-of-session handlers are registered and cleaned up
  </verify>
  <done>
    - Eviction scoring follows formula: (age * weight) / confidence
    - Hard cap of 150 facts enforced with pinned protection
    - Extraction triggers every 5 messages
    - End-of-session extraction on tab close/visibility change
  </done>
</task>

<task type="auto">
  <name>Task 4: Transport Integration</name>
  <files>
    src/lib/ai/chat-transport.ts
  </files>
  <action>
    Integrate memory context into the chat transport:

    1. Locate the chat transport file and identify where system prompts are built

    2. Import from memory modules:
       ```typescript
       import { buildContextWithMemory, compactContext } from "@/lib/memory/context-builder";
       import { getMemoryFacts } from "@/lib/storage/memory";
       ```

    3. Modify the transport to include memory:
       - Before sending messages to the model:
         a) Load all memory facts: getMemoryFacts()
         b) Build context with memory: buildContextWithMemory()
         c) Check if compaction needed: shouldCompact()
         d) If needed, apply: compactContext()
       - Inject formatted memory into system prompt

    4. Add token usage tracking:
       - Track percentUsed from buildContextWithMemory
       - Log in DEV mode when approaching threshold
       - Expose via transport callbacks if needed

    5. Ensure the integration is non-blocking:
       - Memory loading happens async before model call
       - Don't wait for extraction (that's separate)
       - Use cached memory facts

    6. Add error handling:
       - If memory loading fails, continue without memory (graceful degradation)
       - Log error but don't break chat
  </action>
  <verify>
    - Chat transport imports and uses buildContextWithMemory
    - Memory is injected into system prompt
    - Compaction triggers at 80% threshold during chat
    - Transport continues working if memory fails to load
  </verify>
  <done>
    - Memory context integrated into chat transport
    - Token budgeting applied to each chat request
    - Compaction happens automatically when needed
    - Graceful fallback if memory system fails
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Test token budget calculation with various model sizes
2. Verify compaction triggers at correct threshold
3. Check that extraction counter increments properly
4. Ensure eviction respects pinned facts
</verification>

<success_criteria>
- Context builder calculates memory budgets correctly (25% of remaining, 150-500 range)
- Two-stage compaction triggers at 80% threshold
- Stage 1 trims conversation history middle-out
- Stage 2 reduces memory injection (drops identity, halves preferences)
- Extraction triggers every 5 messages and at end-of-session
- Eviction maintains hard cap of 150 facts with pinned protection
</success_criteria>

<output>
After completion, create `.planning/phases/04-memory-system/04-02-SUMMARY.md`
</output>
