---
phase: 02.2-stabilization-and-refactor
plan: 03
type: execute
wave: 2
depends_on: ["02.2-01"]
files_modified:
  - src/hooks/useAIChat.ts
  - src/lib/ai/webllm-transport.ts
  - src/lib/storage/conversations.ts
  - src/lib/utils/debounce.ts
autonomous: true
requirements:
  - TECH-05
  - TECH-06
must_haves:
  truths:
    - Conversation persistence is debounced (500ms)
    - Rapid message updates don't cause race conditions
    - Event listeners in transport are properly cleaned up
    - No memory leaks from uncleaned subscriptions
  artifacts:
    - path: src/lib/utils/debounce.ts
      provides: Debounce utility with cancellation
      exports: ["debounce", "DebouncedFunction"]
    - path: src/hooks/useAIChat.ts
      provides: Debounced persistence effect
      pattern: "useEffect with debounced save"
    - path: src/lib/ai/webllm-transport.ts
      provides: Event listener cleanup
      pattern: "removeEventListener in finally/cleanup"
  key_links:
    - from: src/hooks/useAIChat.ts
      to: src/lib/utils/debounce.ts
      via: import
      pattern: "import.*debounce"
    - from: src/lib/ai/webllm-transport.ts
      to: "AbortSignal cleanup"
      via: "abortSignal.removeEventListener"
---

<objective>
Fix race conditions in conversation persistence and memory leaks in WebLLM transport. Implement debouncing for saves and proper event listener cleanup.

Purpose: Prevent data corruption from rapid saves and eliminate memory leaks from uncleaned event listeners.
Output: Robust persistence with debouncing and leak-free transport layer.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.2-stabilization-and-refactor/02.2-CONTEXT.md
@.planning/audit/STABILITY_AUDIT.md

## Race Condition Issue

From STABILITY_AUDIT.md Section 2.1:
```typescript
// src/hooks/useAIChat.ts:74-120
useEffect(() => {
  if (messages.length === 0) return;

  const persistMessages = async () => {
    // ... persistence logic
    await saveConversation(conversation);
  };

  persistMessages();
}, [messages, conversationId, modelId]);
```

Issue: No debouncing - rapid message updates cause out-of-order writes.

## Memory Leak Issue

From STABILITY_AUDIT.md Section 2.2:
```typescript
// src/lib/ai/webllm-transport.ts:77-84
if (abortSignal) {
  abortSignal.addEventListener("abort", () => {
    this.abortController?.abort();
  });
}
// No removal of event listener!
```

## Decisions from CONTEXT.md

- **Cancellation:** All long-running operations accept AbortSignal
- **Cleanup:** useEffect and event listeners properly disposed
- **Race protection:** Optimistic locking for storage operations
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create debounce utility</name>
  <files>src/lib/utils/debounce.ts</files>
  <action>
Create src/lib/utils/debounce.ts with proper TypeScript types:

```typescript
/**
 * Debounce utility
 *
 * Delays function execution until after wait milliseconds have elapsed
 * since the last time the debounced function was invoked.
 * Includes cancellation support for cleanup.
 */

/**
 * A debounced function with cancel and flush methods
 */
export interface DebouncedFunction<T extends (...args: Parameters<T>) => ReturnType<T>> {
  (...args: Parameters<T>): void;
  /** Cancel pending execution */
  cancel(): void;
  /** Immediately execute pending call */
  flush(): void;
  /** Whether a call is pending */
  readonly pending: boolean;
}

/**
 * Creates a debounced function
 *
 * @param fn - Function to debounce
 * @param wait - Milliseconds to delay (default: 300)
 * @param immediate - Execute on leading edge instead of trailing
 * @returns Debounced function with cancel/flush methods
 */
export function debounce<T extends (...args: unknown[]) => unknown>(
  fn: T,
  wait = 300,
  immediate = false
): DebouncedFunction<T> {
  let timeoutId: ReturnType<typeof setTimeout> | null = null;
  let lastArgs: Parameters<T> | null = null;
  let lastThis: ThisParameterType<T> | null = null;

  const debounced = function (
    this: ThisParameterType<T>,
    ...args: Parameters<T>
  ): void {
    lastArgs = args;
    lastThis = this;

    const callNow = immediate && !timeoutId;

    // Clear existing timeout
    if (timeoutId) {
      clearTimeout(timeoutId);
      timeoutId = null;
    }

    // Schedule new execution
    timeoutId = setTimeout(() => {
      timeoutId = null;
      if (!immediate && lastArgs) {
        fn.apply(lastThis, lastArgs);
        lastArgs = null;
        lastThis = null;
      }
    }, wait);

    // Execute immediately if requested
    if (callNow) {
      fn.apply(lastThis, lastArgs);
      lastArgs = null;
      lastThis = null;
    }
  } as DebouncedFunction<T>;

  debounced.cancel = () => {
    if (timeoutId) {
      clearTimeout(timeoutId);
      timeoutId = null;
    }
    lastArgs = null;
    lastThis = null;
  };

  debounced.flush = () => {
    if (timeoutId) {
      clearTimeout(timeoutId);
      timeoutId = null;
    }
    if (lastArgs) {
      fn.apply(lastThis, lastArgs);
      lastArgs = null;
      lastThis = null;
    }
  };

  Object.defineProperty(debounced, "pending", {
    get: () => timeoutId !== null,
  });

  return debounced;
}

/**
 * Hook-compatible debounce for React
 * Returns a stable callback that debounces the latest function
 */
export function useDebouncedCallback<T extends (...args: unknown[]) => unknown>(
  fn: T,
  wait = 300
): DebouncedFunction<T> {
  // Create debounced function once
  const debouncedRef = useRef<DebouncedFunction<T>>();

  if (!debouncedRef.current) {
    debouncedRef.current = debounce(fn, wait);
  }

  // Update the underlying function when deps change
  useEffect(() => {
    const debounced = debounce(fn, wait);
    debouncedRef.current = debounced;
    return () => {
      debounced.cancel();
    };
  }, [fn, wait]);

  return debouncedRef.current;
}

// Need to import useRef, useEffect for the hook version
import { useRef, useEffect } from "react";
```

This provides both a standalone debounce function and a React hook version.
  </action>
  <verify>grep -q "export function debounce" src/lib/utils/debounce.ts && grep -q "export interface DebouncedFunction" src/lib/utils/debounce.ts</verify>
  <done>Debounce utility created with cancel/flush/pending methods</done>
</task>

<task type="auto">
  <name>Task 2: Fix race condition in useAIChat persistence</name>
  <files>src/hooks/useAIChat.ts</files>
  <action>
Refactor src/hooks/useAIChat.ts to use debounced persistence:

Current code (lines 74-120):
```typescript
// Persist messages to IndexedDB when they change
useEffect(() => {
  if (messages.length === 0) return;

  const persistMessages = async () => {
    try {
      let conversation = await getConversation(conversationId);
      // ... persistence logic
      await saveConversation(conversation);
    } catch (error) {
      console.error("Failed to persist conversation:", error);
    }
  };

  persistMessages();
}, [messages, conversationId, modelId]);
```

Refactored code:
```typescript
import { useEffect, useRef, useCallback } from "react";
import { useChat, type UIMessage } from "@ai-sdk/react";
import type { TextUIPart } from "ai";
import { WebLLMTransport } from "@/lib/ai/webllm-transport";
import { getConversation, saveConversation, createConversation } from "@/lib/storage/conversations";
import type { Message } from "@/types/index";

// Debounce delay for persistence (ms)
const PERSISTENCE_DEBOUNCE_MS = 500;

export interface UseAIChatOptions {
  conversationId: string;
  modelId: string;
  initialMessages?: UIMessage[];
}

export function useAIChat(options: UseAIChatOptions) {
  const { conversationId, modelId, initialMessages } = options;

  // Create WebLLM transport instance with the specified model
  // Memoize to prevent recreating on every render
  const transport = useMemo(() => new WebLLMTransport({ modelId }), [modelId]);

  // Configure useChat with the WebLLM transport
  const chatHelpers = useChat({
    id: conversationId,
    transport,
    messages: initialMessages,
  });

  const { messages } = chatHelpers;

  // Refs for debouncing
  const persistTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const pendingSaveRef = useRef<boolean>(false);

  // Persist messages to IndexedDB when they change (debounced)
  useEffect(() => {
    if (messages.length === 0) return;

    // Clear any pending save
    if (persistTimeoutRef.current) {
      clearTimeout(persistTimeoutRef.current);
    }

    pendingSaveRef.current = true;

    // Schedule new save
    persistTimeoutRef.current = setTimeout(async () => {
      pendingSaveRef.current = false;

      try {
        let conversation = await getConversation(conversationId);

        // If conversation doesn't exist, create it (first message scenario)
        if (!conversation) {
          conversation = createConversation(modelId, conversationId);
        }

        // Update title if this is the first user message
        const firstUserMessage = messages.find((m) => m.role === "user");
        if (firstUserMessage && conversation.title === "New Conversation") {
          const textContent = firstUserMessage.parts
            .filter((p): p is TextUIPart => p.type === "text")
            .map((p) => p.text)
            .join("");
          if (textContent) {
            conversation.title = textContent.slice(0, 50) + (textContent.length > 50 ? "..." : "");
          }
        }

        // Convert UIMessages to storage format
        const storageMessages: Message[] = messages.map((msg): Message => ({
          id: msg.id,
          role: msg.role,
          content: msg.parts
            .filter((p): p is TextUIPart => p.type === "text")
            .map((p) => p.text)
            .join(""),
          timestamp: Date.now(),
          conversationId,
        }));

        conversation.messages = storageMessages;
        conversation.updatedAt = Date.now();

        await saveConversation(conversation);
      } catch (error) {
        if (import.meta.env.DEV) {
          console.error("Failed to persist conversation:", error);
        }
      }
    }, PERSISTENCE_DEBOUNCE_MS);

    // Cleanup on unmount or next effect run
    return () => {
      if (persistTimeoutRef.current) {
        clearTimeout(persistTimeoutRef.current);
      }
    };
  }, [messages, conversationId, modelId]);

  // Flush pending save on unmount
  useEffect(() => {
    return () => {
      if (persistTimeoutRef.current && pendingSaveRef.current) {
        clearTimeout(persistTimeoutRef.current);
        // Note: We can't await here, but the next mount will save current state
      }
    };
  }, []);

  return chatHelpers;
}
```

Key changes:
1. Added 500ms debounce to persistence
2. Clear previous timeout before scheduling new one
3. Cleanup timeout on effect cleanup
4. Track pending save state
5. Add useMemo for transport to prevent recreating every render
  </action>
  <verify>grep -q "PERSISTENCE_DEBOUNCE_MS" src/hooks/useAIChat.ts && grep -q "setTimeout" src/hooks/useAIChat.ts && grep -q "clearTimeout" src/hooks/useAIChat.ts</verify>
  <done>useAIChat persistence debounced with 500ms delay and proper cleanup</done>
</task>

<task type="auto">
  <name>Task 3: Fix memory leak in webllm-transport</name>
  <files>src/lib/ai/webllm-transport.ts</files>
  <action>
Fix the event listener memory leak in src/lib/ai/webllm-transport.ts:

Current code (lines 77-84):
```typescript
// Link with external abort signal if provided
if (abortSignal) {
  abortSignal.addEventListener("abort", () => {
    this.abortController?.abort();
  });
}
```

Problem: Event listener is never removed.

Fixed code:
```typescript
async sendMessages({
  messages,
  abortSignal,
}: {
  trigger: "submit-message" | "regenerate-message";
  chatId: string;
  messageId: string | undefined;
  messages: UIMessage[];
  abortSignal: AbortSignal | undefined;
} & ChatRequestOptions): Promise<ReadableStream<UIMessageChunk>> {
  // Create a new AbortController for this request
  this.abortController = new AbortController();

  // Store cleanup function for abort listener
  let abortCleanup: (() => void) | null = null;

  // Link with external abort signal if provided
  if (abortSignal) {
    const abortHandler = () => {
      this.abortController?.abort();
    };
    abortSignal.addEventListener("abort", abortHandler);
    abortCleanup = () => {
      abortSignal.removeEventListener("abort", abortHandler);
    };
  }

  // Convert UIMessages to WebLLM format
  const webLLMMessages = this.convertToWebLLMMessages(messages);

  // Create the stream
  const stream = new ReadableStream<UIMessageChunk>({
    start: async (controller) => {
      try {
        // Check if model is loaded
        if (!inferenceManager.isLoaded()) {
          throw new Error(
            "Model not initialized. Please load a model before sending messages."
          );
        }

        const messageId = crypto.randomUUID();

        controller.enqueue({
          type: "text-start",
          id: messageId,
        });

        startTokenTracking();

        const tokenStream = inferenceManager.generate(webLLMMessages);

        for await (const token of tokenStream) {
          if (this.abortController?.signal.aborted) {
            break;
          }

          recordToken();

          controller.enqueue({
            type: "text-delta",
            delta: token,
            id: messageId,
          });
        }

        stopTokenTracking();

        controller.enqueue({
          type: "text-end",
          id: messageId,
        });

        controller.close();
      } catch (error) {
        const errorMessage =
          error instanceof Error ? error.message : "Unknown error occurred";

        controller.enqueue({
          type: "error",
          errorText: errorMessage,
        });

        controller.close();
      } finally {
        // Clean up abort listener
        abortCleanup?.();
      }
    },
    cancel: () => {
      // Clean up when the stream is cancelled
      abortCleanup?.();
      this.abort();
    },
  });

  return stream;
}
```

Key changes:
1. Store abort handler as named function
2. Create cleanup function that removes listener
3. Call cleanup in finally block of start
4. Call cleanup in cancel handler
5. This ensures listener is always removed, preventing memory leak
  </action>
  <verify>grep -q "abortCleanup" src/lib/ai/webllm-transport.ts && grep -q "removeEventListener" src/lib/ai/webllm-transport.ts && grep -q "finally" src/lib/ai/webllm-transport.ts</verify>
  <done>Event listener cleanup implemented with abortCleanup function and finally block</done>
</task>

</tasks>

<verification>
- Debounce utility works correctly (test with rapid calls)
- Persistence waits 500ms after last message change
- Event listeners are removed after stream completes
- No memory leaks detected in Chrome DevTools Memory tab
</verification>

<success_criteria>
- Debounce utility created and exported
- useAIChat persistence debounced to 500ms
- Transport event listeners properly cleaned up
- No race conditions in rapid message updates
- No memory leaks from transport event listeners
</success_criteria>

<output>
After completion, create `.planning/phases/02.2-stabilization-and-refactor/02.2-03-SUMMARY.md`
</output>
