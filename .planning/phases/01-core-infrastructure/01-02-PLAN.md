---
phase: 01-core-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/workers/inference.worker.ts
  - src/lib/ai/models.ts
  - src/lib/ai/inference.ts
  - src/store/modelStore.ts
autonomous: true
requirements:
  - MODEL-01
  - MODEL-07
must_haves:
  truths:
    - "Web Worker runs AI inference without blocking main thread"
    - "Quick Mode (Phi-2) auto-loads on first visit"
    - "Model download progress is trackable (0-100%)"
    - "Downloaded models persist in IndexedDB across sessions"
    - "Model state is reactive via Zustand store"
  artifacts:
    - path: "src/workers/inference.worker.ts"
      provides: "Web Worker for AI inference using WebLLM"
      min_lines: 30
    - path: "src/lib/ai/models.ts"
      provides: "Model configurations (Quick/Smart/Genius modes)"
      exports: ["QUICK_MODEL", "MODELS", "getModelById"]
    - path: "src/lib/ai/inference.ts"
      provides: "Inference manager with Web Worker integration"
      exports: ["InferenceManager", "inferenceManager"]
    - path: "src/store/modelStore.ts"
      provides: "Reactive model state with download progress"
      exports: ["useModelStore"]
  key_links:
    - from: "src/lib/ai/inference.ts"
      to: "src/workers/inference.worker.ts"
      via: "Web Worker instantiation"
      pattern: "new Worker.*inference.worker"
    - from: "src/lib/ai/inference.ts"
      to: "src/lib/ai/models.ts"
      via: "Model configuration lookup"
      pattern: "QUICK_MODEL|MODELS"
    - from: "src/store/modelStore.ts"
      to: "src/lib/ai/inference.ts"
      via: "Inference manager integration"
      pattern: "inferenceManager"
---

<objective>
Set up the AI inference infrastructure: Web Worker for non-blocking inference, model configurations for Quick Mode, inference manager with progress tracking, and reactive model state.

Purpose: This is the core AI capability. The Web Worker ensures UI responsiveness during model loading and inference. Quick Mode auto-load provides the 60-second first-message target.
Output: Working Web Worker, model configurations, inference manager singleton, and model store with progress tracking.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/STACK.md
@.planning/research/ARCHITECTURE.md
@.planning/1-CONTEXT.md

Phase 1 Context (locked decisions):
- Quick Mode auto-loads on first visit (no user choice needed for initial load)
- Progress shown: Percentage (0-100%), loading step label, time estimate
- Cancelable: Yes
- Steps: "Downloading Phi-2 model...", "Compiling shaders...", "Ready!"
- Downloaded models persist across browser sessions (cached in IndexedDB)
</context>

<tasks>

<task type="auto">
<name>Task 1: Create Web Worker for AI inference</name>
<files>src/workers/inference.worker.ts</files>
<action>
Create the Web Worker that runs WebLLM inference off the main thread:

1. Import { WebWorkerMLCEngineHandler } from "@mlc-ai/web-llm"
2. Create handler instance: const handler = new WebWorkerMLCEngineHandler()
3. Set up message handler:
   self.onmessage = (msg: MessageEvent) => {
     handler.onmessage(msg);
   };

This is the minimal Web Worker setup per WebLLM documentation. The handler manages all communication between main thread and MLCEngine.

Export nothing (worker runs in isolation). Use type: "module" when instantiating.
</action>
<verify>
grep -q "WebWorkerMLCEngineHandler" src/workers/inference.worker.ts && \
grep -q "self.onmessage" src/workers/inference.worker.ts
</verify>
<done>Web Worker created with WebWorkerMLCEngineHandler, ready for main thread integration</done>
</task>

<task type="auto">
<name>Task 2: Define model configurations</name>
<files>src/lib/ai/models.ts</files>
<action>
Create model configuration definitions:

1. Define ModelConfig interface with:
   - id: string (e.g., "phi-2-q4f16_1")
   - name: string (display name)
   - description: string
   - sizeMB: number (approximate download size)
   - modelUrl: string (HuggingFace or CDN URL)
   - requiredVRAM: number (MB)

2. Export QUICK_MODEL constant:
   - id: "phi-2-q4f16_1"
   - name: "Quick Mode"
   - description: "Fast responses, good for everyday chat (Phi-2 2.7B)"
   - sizeMB: 80
   - modelUrl: Use WebLLM prebuilt config
   - requiredVRAM: 512

3. Export MODELS array with all three model configs (Quick, Smart, Genius placeholders)

4. Export helper functions:
   - getModelById(id: string): ModelConfig | undefined
   - getDefaultModel(): ModelConfig (returns QUICK_MODEL)

Use WebLLM's prebuiltAppConfig for model URLs to leverage their CDN.
</action>
<verify>
grep -q "export const QUICK_MODEL" src/lib/ai/models.ts && \
grep -q "export const MODELS" src/lib/ai/models.ts && \
grep -q "export function getModelById" src/lib/ai/models.ts && \
grep -q "phi-2" src/lib/ai/models.ts
</verify>
<done>Model configurations defined with Quick Mode (Phi-2) as default, 80MB size specified</done>
</task>

<task type="auto">
<name>Task 3: Create inference manager with progress tracking</name>
<files>src/lib/ai/inference.ts</files>
<action>
Create the InferenceManager class that orchestrates AI inference:

1. Define InferenceManager class with private properties:
   - engine: MLCEngineInterface | null
   - worker: Worker | null
   - currentModel: string | null
   - progressCallback: ((progress: DownloadProgress) => void) | null

2. Implement async initialize(modelId: string, onProgress?: (progress: DownloadProgress) => void):
   - Create Web Worker using new URL("../workers/inference.worker.ts", import.meta.url)
   - Call CreateWebWorkerMLCEngine(worker, modelId, { initProgressCallback })
   - Calculate progress percentage from report.loaded / report.total
   - Estimate time remaining based on download speed (rolling average)
   - Call onProgress with { loaded, total, percentage, estimatedTimeSeconds }
   - Store engine and currentModel on success

3. Implement async generate(messages: Message[]): AsyncGenerator<string>:
   - Check engine is initialized
   - Call engine.chat.completions.create({ messages, stream: true, temperature: 0.7 })
   - Yield each token from the stream
   - Handle abort/reset

4. Implement abort(): void to interrupt generation

5. Export singleton instance: inferenceManager

Follow ARCHITECTURE.md Pattern 1 (Web Worker Offloading) and Pattern 2 (Service Layer Abstraction).
</action>
<verify>
grep -q "export class InferenceManager" src/lib/ai/inference.ts && \
grep -q "async initialize" src/lib/ai/inference.ts && \
grep -q "async.*generate" src/lib/ai/inference.ts && \
grep -q "export const inferenceManager" src/lib/ai/inference.ts && \
grep -q "CreateWebWorkerMLCEngine" src/lib/ai/inference.ts
</verify>
<done>Inference manager created with Web Worker integration, progress tracking, and streaming generation</done>
</task>

<task type="auto">
<name>Task 4: Create model Zustand store</name>
<files>src/store/modelStore.ts</files>
<action>
Create reactive model state store:

1. Define ModelState interface with:
   - currentModel: ModelConfig | null
   - isLoading: boolean
   - downloadProgress: DownloadProgress | null
   - loadingStep: "idle" | "downloading" | "compiling" | "ready" | "error"
   - error: string | null
   - availableModels: ModelConfig[]

2. Create store with Zustand:
   - Initialize with QUICK_MODEL as currentModel (null until loaded)
   - loadingStep: "idle"
   - availableModels: MODELS

3. Implement actions:
   - loadModel(modelId: string): Async action that:
     * Sets isLoading: true, loadingStep: "downloading"
     * Calls inferenceManager.initialize with progress callback
     * Updates downloadProgress in real-time
     * On progress > 95%, set loadingStep: "compiling"
     * On complete, set loadingStep: "ready", currentModel
     * On error, set loadingStep: "error", error message

   - cancelDownload(): Calls inferenceManager.abort(), resets state

   - resetModel(): Unloads current model, resets to idle

4. Export useModelStore hook

Store should integrate with inferenceManager for actual operations.
</action>
<verify>
grep -q "export const useModelStore" src/store/modelStore.ts && \
grep -q "loadModel" src/store/modelStore.ts && \
grep -q "downloadProgress" src/store/modelStore.ts && \
grep -q "loadingStep" src/store/modelStore.ts
</verify>
<done>Model store created with reactive state for loading, progress, and error handling</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. TypeScript compiles without errors
2. Web Worker can be instantiated without errors
3. Model configurations include Quick Mode (Phi-2, ~80MB)
4. Inference manager can track download progress (0-100%)
5. Model store integrates with inference manager
6. No main thread blocking during initialization (Web Worker pattern)
</verification>

<success_criteria>
- Web Worker created with WebWorkerMLCEngineHandler
- Model configurations define Quick Mode (Phi-2, 80MB)
- Inference manager handles Web Worker creation and progress callbacks
- Model store provides reactive state with download progress
- TypeScript compilation passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-infrastructure/01-02-SUMMARY.md` documenting:
- Web Worker architecture
- Model configurations (Quick/Smart/Genius)
- Inference manager API
- Model store state shape
</output>
