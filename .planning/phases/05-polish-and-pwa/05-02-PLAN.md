---
phase: 05-polish-and-pwa
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/components/Chat/AIChatInterface.tsx
  - src/components/Chat/conversation-history-window.tsx
  - src/routes/ChatDetailRoute.tsx
autonomous: true
requirements:
  - CHAT-10
must_haves:
  truths:
    - "Conversations with more than 50 messages load older history in chunks while scrolling upward"
    - "Opening an existing conversation lands at the latest message by default"
    - "Existing streaming auto-scroll behavior remains unchanged"
  artifacts:
    - path: "src/components/Chat/conversation-history-window.tsx"
      provides: "Chunk window state and scroll-preserving prepend logic"
      exports: ["useConversationHistoryWindow", "CHUNK_SIZE"]
    - path: "src/components/Chat/AIChatInterface.tsx"
      provides: "Integration of bounded history rendering with current conversation UI"
      exports: ["AIChatInterface"]
    - path: "src/routes/ChatDetailRoute.tsx"
      provides: "Bottom-first entry signal when opening old conversations"
      exports: ["ChatDetailRoute"]
  key_links:
    - from: "src/components/Chat/AIChatInterface.tsx"
      to: "src/components/Chat/conversation-history-window.tsx"
      via: "visible message slice + load older trigger"
      pattern: "useConversationHistoryWindow"
    - from: "src/routes/ChatDetailRoute.tsx"
      to: "src/components/Chat/AIChatInterface.tsx"
      via: "prop indicating initial bottom landing behavior"
      pattern: "startAtBottom|initialStart"
---

<objective>
Deliver long-thread browsing behavior for 50+ message chats using upward chunk loading while preserving current streaming UX and bottom-stick semantics.

Purpose: CHAT-10 requires smooth long-conversation performance without changing the already-approved streaming and position-jump behavior.
Output: Chunked history window logic integrated into AI chat rendering and route entry behavior.
</objective>

<execution_context>
@/Users/poak/.config/opencode/get-shit-done/workflows/execute-plan.md
@/Users/poak/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-polish-and-pwa/05-CONTEXT.md
@.planning/phases/05-polish-and-pwa/05-RESEARCH.md
@src/components/Chat/AIChatInterface.tsx
@src/components/ai-elements/conversation.tsx
@src/routes/ChatDetailRoute.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement reusable chunk-window history controller</name>
  <files>
    src/components/Chat/conversation-history-window.tsx
  </files>
  <action>
    Create a focused history-window module that activates only when `messages.length > 50`. Implement tail-first initialization (latest chunk visible), upward prepend when near top, and scroll offset compensation so prepending older chunks does not cause jumpy reading position. Keep API small and testable (input messages + scroll metrics -> visible range and load handlers).
  </action>
  <verify>
    - npm run type-check
    - For synthetic 80+ message data, initial visible range starts at latest chunk
    - Prepending older chunks preserves visual position
  </verify>
  <done>
    - Chunk controller exists with clear threshold and chunk-size behavior
    - Upward loading logic prevents scroll jump regressions
    - Module is independent from UI styling details
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate chunked history into AIChatInterface without streaming regressions</name>
  <files>
    src/components/Chat/AIChatInterface.tsx
    src/components/Chat/conversation-history-window.tsx
  </files>
  <action>
    Wire AIChatInterface to render a bounded message slice from the new history controller and trigger older-chunk loads when scrolling near top. Preserve current streaming path, ConversationScrollButton behavior, and any existing stick-to-bottom semantics for active generation. Do not introduce new long-thread jump UI in this phase.
  </action>
  <verify>
    - npm run type-check
    - Stream a new response in long conversation and confirm current auto-scroll behavior matches pre-phase behavior
    - Scroll upward in long conversation and confirm older chunks load incrementally
  </verify>
  <done>
    - AIChatInterface uses chunked rendering for long histories
    - Streaming UX remains unchanged for active generations
    - Existing position/jump UI is untouched
  </done>
</task>

<task type="auto">
  <name>Task 3: Ensure old-conversation route entry lands at latest message</name>
  <files>
    src/routes/ChatDetailRoute.tsx
    src/components/Chat/AIChatInterface.tsx
  </files>
  <action>
    Ensure opening `/chat/:id` initializes the history window at the bottom (latest message) by default for existing conversations. Pass any explicit route-level entry hint needed by AIChatInterface, and keep behavior stable for new/empty conversations.
  </action>
  <verify>
    - npm run type-check
    - Open an older long conversation via sidebar and confirm initial viewport lands at latest messages
    - Open a new conversation and confirm empty state remains unchanged
  </verify>
  <done>
    - Existing conversation open flow starts from most recent messages
    - New and empty conversation behavior remains correct
    - Route and chat component stay type-safe
  </done>
</task>

</tasks>

<verification>
1. Validate long-thread behavior with at least one conversation containing more than 50 messages.
2. Confirm upward loading performance and scroll-position stability.
3. Confirm streaming behavior is unchanged relative to current baseline.
</verification>

<success_criteria>
- CHAT-10 behavior is implemented with chunked upward history loading.
- Old conversation open starts at bottom/latest message.
- No regressions in current streaming and scroll-button behavior.
</success_criteria>

<output>
After completion, create `.planning/phases/05-polish-and-pwa/05-02-SUMMARY.md`
</output>
